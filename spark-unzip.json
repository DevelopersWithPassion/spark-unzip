{"paragraphs":[{"title":"As always, check the Spark version","text":"sc.version","dateUpdated":"2016-11-13T18:06:39+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476888496349_736584997","id":"20161019-164816_1106641995","result":{"code":"SUCCESS","type":"TEXT","msg":"res29: String = 1.6.2\n"},"dateCreated":"2016-10-19T16:48:16+0200","dateStarted":"2016-11-13T18:06:39+0100","dateFinished":"2016-11-13T18:06:39+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5124"},{"text":"%md\n# 1) ZIP compressed data\n\nZIP compression format is not splittable and there is no default input format defined in Hadoop. To read ZIP files, Hadoop needs to be informed that it this file type is not splittable and needs an appropriate record reader, see [Hadoop: Processing ZIP files in Map/Reduce](http://cutler.io/2012/07/hadoop-processing-zip-files-in-mapreduce/).\n\nIn order to work with ZIP files in Zeppelin, follow the installation instructions in the appendix of this notebook\n\n","dateUpdated":"2016-11-13T18:06:41+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479041247706_-1949630272","id":"20161113-134727_9559059","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>1) ZIP compressed data</h1>\n<p>ZIP compression format is not splittable and there is no default input format defined in Hadoop. To read ZIP files, Hadoop needs to be informed that it this file type is not splittable and needs an appropriate record reader, see <a href=\"http://cutler.io/2012/07/hadoop-processing-zip-files-in-mapreduce/\">Hadoop: Processing ZIP files in Map/Reduce</a>.</p>\n<p>In order to work with ZIP files in Zeppelin, follow the installation instructions in the appendix of this notebook</p>\n"},"dateCreated":"2016-11-13T13:47:27+0100","dateStarted":"2016-11-13T18:06:41+0100","dateFinished":"2016-11-13T18:06:41+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6595"},{"title":"Six zip files containing XML records are placed below /tmp/zip","text":"%sh\n\necho \"Folder:\"\nhdfs dfs -ls /tmp/zip\necho \" \"\necho \"XML records:\"\nhdfs dfs -get /tmp/zip/logfile_1.zip /tmp/l.zip\nunzip -p /tmp/l.zip | head -n 3\n","dateUpdated":"2016-11-13T18:06:44+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479044260709_-449418287","id":"20161113-143740_1188005968","result":{"code":"SUCCESS","type":"TEXT","msg":"Folder:\nFound 6 items\n-rw-r--r--   3 bernhard hdfs       3764 2016-11-11 17:55 /tmp/zip/logfile_1.zip\n-rw-r--r--   3 bernhard hdfs       3833 2016-11-11 17:55 /tmp/zip/logfile_2.zip\n-rw-r--r--   3 bernhard hdfs       3796 2016-11-11 17:55 /tmp/zip/logfile_3.zip\n-rw-r--r--   3 bernhard hdfs       3757 2016-11-11 17:55 /tmp/zip/logfile_4.zip\n-rw-r--r--   3 bernhard hdfs       3841 2016-11-11 17:55 /tmp/zip/logfile_5.zip\n-rw-r--r--   3 bernhard hdfs       3712 2016-11-11 17:55 /tmp/zip/logfile_6.zip\n \nXML records:\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><name type=\"dict\"><first_name type=\"str\">Heinz Dieter</first_name><name type=\"str\">Antonino Döhn</name></name><address type=\"dict\"><city type=\"str\">Iserlohn</city><street type=\"str\">Erich-Roskoth-Straße</street><country type=\"str\">Deutschland</country></address></root>\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><name type=\"dict\"><first_name type=\"str\">Marta</first_name><name type=\"str\">Hedy Kobelt</name></name><address type=\"dict\"><city type=\"str\">Jena</city><street type=\"str\">Slavko-Karge-Gasse</street><country type=\"str\">Deutschland</country></address></root>\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><name type=\"dict\"><first_name type=\"str\">Amalia</first_name><name type=\"str\">Sibylle Jacobi Jäckel</name></name><address type=\"dict\"><city type=\"str\">Böblingen</city><street type=\"str\">Teresa-Meister-Allee</street><country type=\"str\">Deutschland</country></address></root>\n"},"dateCreated":"2016-11-13T14:37:40+0100","dateStarted":"2016-11-13T18:06:44+0100","dateFinished":"2016-11-13T18:06:56+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6661"},{"title":"Empty the results folder","text":"%sh\nhdfs dfs -rm -r /tmp/results\nhdfs dfs -mkdir /tmp/results","dateUpdated":"2016-11-13T18:07:01+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh","tableHide":false,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1478972827127_-2060293394","id":"20161112-184707_1443886968","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-11-12T18:47:07+0100","dateStarted":"2016-11-13T18:07:01+0100","dateFinished":"2016-11-13T18:07:12+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6731"},{"title":"Import all necessary symbols","text":"import com.cotdp.hadoop.ZipFileInputFormat\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\nimport org.apache.hadoop.io.{BytesWritable, Text}\nimport org.apache.spark.{SparkConf, SparkContext}\nimport scala.xml.XML","dateUpdated":"2016-11-13T18:07:18+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1478536196808_-155352920","id":"20161107-172956_1447712391","result":{"code":"SUCCESS","type":"TEXT","msg":"import com.cotdp.hadoop.ZipFileInputFormat\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\nimport org.apache.hadoop.io.{BytesWritable, Text}\nimport org.apache.spark.{SparkConf, SparkContext}\nimport scala.xml.XML\n"},"dateCreated":"2016-11-07T17:29:56+0100","dateStarted":"2016-11-13T18:07:18+0100","dateFinished":"2016-11-13T18:07:19+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6801"},{"title":"Parse an XML record and return it as a scala case class instance","text":"case class Person(first_name:String, name:String, street:String, city:String, country:String)\n\ndef parseXML(xmlStr: String) = {\n    val xml = XML.loadString(xmlStr)\n    val first_name = (xml \\ \"name\" \\ \"first_name\").text\n    val name = (xml \\ \"name\" \\ \"name\").text\n    val street = (xml \\ \"address\" \\ \"street\").text\n    val city = (xml \\ \"address\" \\ \"city\").text\n    val country = (xml \\ \"address\" \\ \"country\").text\n\n    Person(first_name, name, street, city, country)\n}","dateUpdated":"2016-11-13T18:07:29+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","tableHide":false,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1478536200215_-796374713","id":"20161107-173000_1460511438","result":{"code":"SUCCESS","type":"TEXT","msg":"defined class Person\nparseXML: (xmlStr: String)Person\n"},"dateCreated":"2016-11-07T17:30:00+0100","dateStarted":"2016-11-13T18:07:29+0100","dateFinished":"2016-11-13T18:07:30+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6871"},{"title":"Read all zip files into an RDD ... ","text":"val zipFileRDD = sc.newAPIHadoopFile(\"/tmp/zip\", classOf[ZipFileInputFormat],\n                                                 classOf[Text], \n                                                 classOf[BytesWritable], \n                                                 sc.hadoopConfiguration)\n                   .map { case(a,b) => new String( b.getBytes(), \"UTF-8\" ) }\n\n","dateUpdated":"2016-11-13T18:07:54+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","tableHide":false,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1478536549683_129237551","id":"20161107-173549_969433631","result":{"code":"SUCCESS","type":"TEXT","msg":"zipFileRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[40] at map at <console>:46\n"},"dateCreated":"2016-11-07T17:35:49+0100","dateStarted":"2016-11-13T18:07:54+0100","dateFinished":"2016-11-13T18:07:55+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6941"},{"title":"... and parse them into a DataFrame using the case class","text":"val df = zipFileRDD.flatMap { _.split(\"\\n\") }\n                   .map(parseXML)\n                   .toDF\n","dateUpdated":"2016-11-13T18:08:11+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479045477918_-445610192","id":"20161113-145757_916046809","result":{"code":"SUCCESS","type":"TEXT","msg":"df: org.apache.spark.sql.DataFrame = [first_name: string, name: string, street: string, city: string, country: string]\n"},"dateCreated":"2016-11-13T14:57:57+0100","dateStarted":"2016-11-13T18:08:07+0100","dateFinished":"2016-11-13T18:08:08+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7011"},{"title":"Show the result","text":"df.show(10)\n","dateUpdated":"2016-11-13T18:08:12+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479044524130_-1635631439","id":"20161113-144204_214502717","result":{"code":"SUCCESS","type":"TEXT","msg":"+------------+--------------------+--------------------+----------+-----------+\n|  first_name|                name|              street|      city|    country|\n+------------+--------------------+--------------------+----------+-----------+\n|Heinz Dieter|       Antonino Döhn|Erich-Roskoth-Straße|  Iserlohn|Deutschland|\n|       Marta|         Hedy Kobelt|  Slavko-Karge-Gasse|      Jena|Deutschland|\n|      Amalia|Sibylle Jacobi Jä...|Teresa-Meister-Allee| Böblingen|Deutschland|\n|      Gerwin|   Pauline Kade B.A.|  Niels-Winkler-Ring|Biedenkopf|Deutschland|\n|        Ines|Norma Binner-Margraf|           Meyerring|   Wolgast|Deutschland|\n|     Cynthia|       Emmerich Metz|  Sandy-Atzler-Allee|  Grafenau|Deutschland|\n|   Felicitas|Frau Cosima Döhn ...|Brunhilde-Naser-Ring| Gadebusch|Deutschland|\n|    Hannchen|Ulrike Bender-Sei...|Frida-Möchlichen-...|    Döbeln|Deutschland|\n|    Frithjof| Niklas Linke B.Eng.|         Lindauplatz| Sternberg|Deutschland|\n|     Chantal|Heinfried Roskoth...|Dorothee-Rosemann...|  Burgdorf|Deutschland|\n+------------+--------------------+--------------------+----------+-----------+\nonly showing top 10 rows\n\n"},"dateCreated":"2016-11-13T14:42:04+0100","dateStarted":"2016-11-13T18:08:12+0100","dateFinished":"2016-11-13T18:08:13+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7081"},{"title":"Save the records as ORC and as Parquet files","text":"df.write.format(\"orc\").save(\"/tmp/results/people.orc\")\ndf.write.format(\"parquet\").save(\"/tmp/results/people.parquet\")","dateUpdated":"2016-11-13T18:08:19+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1478536653983_165373300","id":"20161107-173733_135405043","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-11-07T17:37:33+0100","dateStarted":"2016-11-13T18:08:19+0100","dateFinished":"2016-11-13T18:08:23+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7151"},{"title":"According to the number of partitions in Spark several files will be created by Spark","text":"%sh\nhdfs dfs -ls /tmp/results/people.orc\nhdfs dfs -ls /tmp/results/people.parquet","dateUpdated":"2016-11-13T18:08:26+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1478972702809_448853398","id":"20161112-184502_185340409","result":{"code":"SUCCESS","type":"TEXT","msg":"Found 7 items\n-rw-r--r--   3 zeppelin hdfs          0 2016-11-13 18:08 /tmp/results/people.orc/_SUCCESS\n-rw-r--r--   3 zeppelin hdfs       4036 2016-11-13 18:08 /tmp/results/people.orc/part-r-00000-465ac1ad-3a48-4b15-9601-b066af702212.orc\n-rw-r--r--   3 zeppelin hdfs       4090 2016-11-13 18:08 /tmp/results/people.orc/part-r-00001-465ac1ad-3a48-4b15-9601-b066af702212.orc\n-rw-r--r--   3 zeppelin hdfs       4051 2016-11-13 18:08 /tmp/results/people.orc/part-r-00002-465ac1ad-3a48-4b15-9601-b066af702212.orc\n-rw-r--r--   3 zeppelin hdfs       4014 2016-11-13 18:08 /tmp/results/people.orc/part-r-00003-465ac1ad-3a48-4b15-9601-b066af702212.orc\n-rw-r--r--   3 zeppelin hdfs       4125 2016-11-13 18:08 /tmp/results/people.orc/part-r-00004-465ac1ad-3a48-4b15-9601-b066af702212.orc\n-rw-r--r--   3 zeppelin hdfs       4027 2016-11-13 18:08 /tmp/results/people.orc/part-r-00005-465ac1ad-3a48-4b15-9601-b066af702212.orc\nFound 9 items\n-rw-r--r--   3 zeppelin hdfs          0 2016-11-13 18:08 /tmp/results/people.parquet/_SUCCESS\n-rw-r--r--   3 zeppelin hdfs        538 2016-11-13 18:08 /tmp/results/people.parquet/_common_metadata\n-rw-r--r--   3 zeppelin hdfs       4448 2016-11-13 18:08 /tmp/results/people.parquet/_metadata\n-rw-r--r--   3 zeppelin hdfs       4913 2016-11-13 18:08 /tmp/results/people.parquet/part-r-00000-1b9890ba-1b08-4905-8826-911b5b6cc8b1.gz.parquet\n-rw-r--r--   3 zeppelin hdfs       4954 2016-11-13 18:08 /tmp/results/people.parquet/part-r-00001-1b9890ba-1b08-4905-8826-911b5b6cc8b1.gz.parquet\n-rw-r--r--   3 zeppelin hdfs       4933 2016-11-13 18:08 /tmp/results/people.parquet/part-r-00002-1b9890ba-1b08-4905-8826-911b5b6cc8b1.gz.parquet\n-rw-r--r--   3 zeppelin hdfs       4897 2016-11-13 18:08 /tmp/results/people.parquet/part-r-00003-1b9890ba-1b08-4905-8826-911b5b6cc8b1.gz.parquet\n-rw-r--r--   3 zeppelin hdfs       4957 2016-11-13 18:08 /tmp/results/people.parquet/part-r-00004-1b9890ba-1b08-4905-8826-911b5b6cc8b1.gz.parquet\n-rw-r--r--   3 zeppelin hdfs       4844 2016-11-13 18:08 /tmp/results/people.parquet/part-r-00005-1b9890ba-1b08-4905-8826-911b5b6cc8b1.gz.parquet\n"},"dateCreated":"2016-11-12T18:45:02+0100","dateStarted":"2016-11-13T18:08:26+0100","dateFinished":"2016-11-13T18:08:38+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7221"},{"title":"Now repartition the DataFrame and store it again as ORC and Parquet","text":" df.repartition(1).write.format(\"orc\").save(\"/tmp/results/people1.orc\")\n df.repartition(1).write.format(\"parquet\").save(\"/tmp/results/people1.parquet\")","dateUpdated":"2016-11-13T18:08:41+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1478972994204_-1040511239","id":"20161112-184954_1381939131","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-11-12T18:49:54+0100","dateStarted":"2016-11-13T18:08:41+0100","dateFinished":"2016-11-13T18:08:45+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7291"},{"title":"There is one content file per saved ORC or Parquet folder in HDFS now","text":"%sh\n\nhdfs dfs -ls /tmp/results/people1.orc\nhdfs dfs -ls /tmp/results/people1.parquet","dateUpdated":"2016-11-13T18:08:48+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1478973035283_-675507126","id":"20161112-185035_1968360301","result":{"code":"SUCCESS","type":"TEXT","msg":"Found 2 items\n-rw-r--r--   3 zeppelin hdfs          0 2016-11-13 18:08 /tmp/results/people1.orc/_SUCCESS\n-rw-r--r--   3 zeppelin hdfs      16601 2016-11-13 18:08 /tmp/results/people1.orc/part-r-00000-099f17e8-7e93-4e3a-9cba-95d8c1e013b4.orc\nFound 4 items\n-rw-r--r--   3 zeppelin hdfs          0 2016-11-13 18:08 /tmp/results/people1.parquet/_SUCCESS\n-rw-r--r--   3 zeppelin hdfs        538 2016-11-13 18:08 /tmp/results/people1.parquet/_common_metadata\n-rw-r--r--   3 zeppelin hdfs       1197 2016-11-13 18:08 /tmp/results/people1.parquet/_metadata\n-rw-r--r--   3 zeppelin hdfs      18288 2016-11-13 18:08 /tmp/results/people1.parquet/part-r-00000-29a86cbd-79cb-4933-86d9-78e0fe06148c.gz.parquet\n"},"dateCreated":"2016-11-12T18:50:35+0100","dateStarted":"2016-11-13T18:08:48+0100","dateFinished":"2016-11-13T18:09:00+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7361"},{"text":"%md\n# 2) GZIP compressed data\n\nFor gzip compressed data a default File input Format exists in Hadoop\n","dateUpdated":"2016-11-13T14:53:49+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","tableHide":false,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479044498548_-1948078194","id":"20161113-144138_759928636","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>2) GZIP compressed data</h1>\n<p>For gzip compressed data a default File input Format exists in Hadoop</p>\n"},"dateCreated":"2016-11-13T14:41:38+0100","dateStarted":"2016-11-13T14:53:46+0100","dateFinished":"2016-11-13T14:53:46+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7431"},{"text":"%sh\n\necho \"Folder:\"\nhdfs dfs -ls /tmp/gzip\necho \" \"\necho \"XML records:\"\nhdfs dfs -cat /tmp/gzip/logfile_1.gz | gzip -dc | head -n 3","dateUpdated":"2016-11-13T18:09:07+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479045232015_-1150606450","id":"20161113-145352_1878739511","result":{"code":"SUCCESS","type":"TEXT","msg":"Folder:\nFound 6 items\n-rw-r--r--   3 bernhard hdfs       3564 2016-11-11 18:21 /tmp/gzip/logfile_1.gz\n-rw-r--r--   3 bernhard hdfs       3632 2016-11-11 18:21 /tmp/gzip/logfile_2.gz\n-rw-r--r--   3 bernhard hdfs       3609 2016-11-11 18:21 /tmp/gzip/logfile_3.gz\n-rw-r--r--   3 bernhard hdfs       3561 2016-11-11 18:21 /tmp/gzip/logfile_4.gz\n-rw-r--r--   3 bernhard hdfs       3638 2016-11-11 18:21 /tmp/gzip/logfile_5.gz\n-rw-r--r--   3 bernhard hdfs       3514 2016-11-11 18:21 /tmp/gzip/logfile_6.gz\n \nXML records:\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><name type=\"dict\"><first_name type=\"str\">Heinz Dieter</first_name><name type=\"str\">Antonino Döhn</name></name><address type=\"dict\"><city type=\"str\">Iserlohn</city><street type=\"str\">Erich-Roskoth-Straße</street><country type=\"str\">Deutschland</country></address></root>\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><name type=\"dict\"><first_name type=\"str\">Marta</first_name><name type=\"str\">Hedy Kobelt</name></name><address type=\"dict\"><city type=\"str\">Jena</city><street type=\"str\">Slavko-Karge-Gasse</street><country type=\"str\">Deutschland</country></address></root>\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><name type=\"dict\"><first_name type=\"str\">Amalia</first_name><name type=\"str\">Sibylle Jacobi Jäckel</name></name><address type=\"dict\"><city type=\"str\">Böblingen</city><street type=\"str\">Teresa-Meister-Allee</street><country type=\"str\">Deutschland</country></address></root>\n"},"dateCreated":"2016-11-13T14:53:52+0100","dateStarted":"2016-11-13T18:09:08+0100","dateFinished":"2016-11-13T18:09:20+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7497"},{"title":"Read all gzip files into an RDD ...","text":"val zipFileRDD2 = sc.textFile(\"/tmp/gzip\")\n","dateUpdated":"2016-11-13T18:09:21+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479045343043_-839793925","id":"20161113-145543_492894261","result":{"code":"SUCCESS","type":"TEXT","msg":"zipFileRDD2: org.apache.spark.rdd.RDD[String] = /tmp/gzip MapPartitionsRDD[56] at textFile at <console>:42\n"},"dateCreated":"2016-11-13T14:55:43+0100","dateStarted":"2016-11-13T18:09:21+0100","dateFinished":"2016-11-13T18:09:21+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7567"},{"title":"... and (as above for zip files) parse them into a DataFrame using the case class","text":"val df2 = zipFileRDD2.flatMap { _.split(\"\\n\") }\n                     .map(parseXML)\n                     .toDF\n                   ","dateUpdated":"2016-11-13T18:09:35+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479045549298_-1397019246","id":"20161113-145909_661176571","result":{"code":"SUCCESS","type":"TEXT","msg":"df2: org.apache.spark.sql.DataFrame = [first_name: string, name: string, street: string, city: string, country: string]\n"},"dateCreated":"2016-11-13T14:59:09+0100","dateStarted":"2016-11-13T18:09:35+0100","dateFinished":"2016-11-13T18:09:36+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7637"},{"text":"df2.show(10)","dateUpdated":"2016-11-13T18:09:40+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479045390697_1188879756","id":"20161113-145630_1813977541","result":{"code":"SUCCESS","type":"TEXT","msg":"+------------+--------------------+--------------------+----------+-----------+\n|  first_name|                name|              street|      city|    country|\n+------------+--------------------+--------------------+----------+-----------+\n|Heinz Dieter|       Antonino Döhn|Erich-Roskoth-Straße|  Iserlohn|Deutschland|\n|       Marta|         Hedy Kobelt|  Slavko-Karge-Gasse|      Jena|Deutschland|\n|      Amalia|Sibylle Jacobi Jä...|Teresa-Meister-Allee| Böblingen|Deutschland|\n|      Gerwin|   Pauline Kade B.A.|  Niels-Winkler-Ring|Biedenkopf|Deutschland|\n|        Ines|Norma Binner-Margraf|           Meyerring|   Wolgast|Deutschland|\n|     Cynthia|       Emmerich Metz|  Sandy-Atzler-Allee|  Grafenau|Deutschland|\n|   Felicitas|Frau Cosima Döhn ...|Brunhilde-Naser-Ring| Gadebusch|Deutschland|\n|    Hannchen|Ulrike Bender-Sei...|Frida-Möchlichen-...|    Döbeln|Deutschland|\n|    Frithjof| Niklas Linke B.Eng.|         Lindauplatz| Sternberg|Deutschland|\n|     Chantal|Heinfried Roskoth...|Dorothee-Rosemann...|  Burgdorf|Deutschland|\n+------------+--------------------+--------------------+----------+-----------+\nonly showing top 10 rows\n\n"},"dateCreated":"2016-11-13T14:56:30+0100","dateStarted":"2016-11-13T18:09:40+0100","dateFinished":"2016-11-13T18:09:41+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7707"},{"text":"%md\n\n# Appendix) Add ZipFileInputFormat to HDP 2.5\n\n## Build ZipFileInputFormat\n\nIn order to get the ZIP Input Format running with HDP 2.5, the repository needs to be downloaded first (`git clone https://github.com/cotdp/com-cotdp-hadoop.git`)\n\nThen `pom.xml` needs to updated to reflect the library versions of HDP 2.5:\n\n```xml\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n    xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n    <groupId>com.cotdp.hadoop</groupId>\n    <artifactId>com-cotdp-hadoop</artifactId>\n    <packaging>jar</packaging>\n    <version>1.0-SNAPSHOT</version>\n    <name>com-cotdp-hadoop</name>\n    <url>http://cotdp.com/</url>\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n    </properties>\n    <repositories>\n        <!-- HDP Repositories -->\n        <repository>\n            <releases>\n                <enabled>true</enabled>\n                <updatePolicy>always</updatePolicy>\n                <checksumPolicy>warn</checksumPolicy>\n            </releases>\n            <snapshots>\n            <enabled>false</enabled>\n            <updatePolicy>never</updatePolicy>\n            <checksumPolicy>fail</checksumPolicy>\n            </snapshots>\n            <id>HDPReleases</id>\n            <name>HDP Releases</name>\n            <url>http://repo.hortonworks.com/content/repositories/releases/</url>\n            <layout>default</layout>\n        </repository>\n        <repository>\n            <releases>\n                <enabled>true</enabled>\n                <updatePolicy>always</updatePolicy>\n                <checksumPolicy>warn</checksumPolicy>\n            </releases>\n            <snapshots>\n                <enabled>true</enabled>\n                <updatePolicy>never</updatePolicy>\n                <checksumPolicy>fail</checksumPolicy>\n            </snapshots>\n            <id>HDPPublic</id>\n            <name>HDP Public</name>\n            <url>http://repo.hortonworks.com/content/repositories/public/</url>\n            <layout>default</layout>\n        </repository>\n    </repositories>\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-common</artifactId>\n            <version>2.7.3.2.5.0.0-1245</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-client</artifactId>\n            <version>2.7.3.2.5.0.0-1245</version>\n        </dependency>\n        <dependency>\n            <groupId>org.codehaus.jackson</groupId>\n            <artifactId>jackson-mapper-asl</artifactId>\n        <version>1.9.8</version>\n        </dependency>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>3.8.1</version>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n</project>\n```\n\nFinally it can be compiled and packaged:\n\n```shell\nmvn package\n```\n\n## Add ZipFileInputFormat to Apache Zeppelin\n\nNow copy `target/com-cotdp-hadoop-1.0-SNAPSHOT.jar` to the machine where Zeppeliln Server is installed, e.g. to `/tmp/com-cotdp-hadoop-1.0-SNAPSHOT.jar`\n\nOpen the Interpreter settings under `<logged-in-user> - Interpreter`, edit `/tmp/com-cotdp-hadoop-1.0-SNAPSHOT.jar` as an artifact below `Dependencies`\n\nRestart the Spark Interpreter and `ZipFileInputformat` can be used\n","dateUpdated":"2016-11-13T15:02:01+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1478973051218_1783453743","id":"20161112-185051_710211259","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Appendix) Add ZipFileInputFormat to HDP 2.5</h1>\n<h2>Build ZipFileInputFormat</h2>\n<p>In order to get the ZIP Input Format running with HDP 2.5, the repository needs to be downloaded first (<code>git clone https://github.com/cotdp/com-cotdp-hadoop.git</code>)</p>\n<p>Then <code>pom.xml</code> needs to updated to reflect the library versions of HDP 2.5:</p>\n<pre><code class=\"xml\">&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n    xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n    &lt;groupId&gt;com.cotdp.hadoop&lt;/groupId&gt;\n    &lt;artifactId&gt;com-cotdp-hadoop&lt;/artifactId&gt;\n    &lt;packaging&gt;jar&lt;/packaging&gt;\n    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\n    &lt;name&gt;com-cotdp-hadoop&lt;/name&gt;\n    &lt;url&gt;http://cotdp.com/&lt;/url&gt;\n    &lt;properties&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n    &lt;/properties&gt;\n    &lt;repositories&gt;\n        &lt;!-- HDP Repositories --&gt;\n        &lt;repository&gt;\n            &lt;releases&gt;\n                &lt;enabled&gt;true&lt;/enabled&gt;\n                &lt;updatePolicy&gt;always&lt;/updatePolicy&gt;\n                &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt;\n            &lt;/releases&gt;\n            &lt;snapshots&gt;\n            &lt;enabled&gt;false&lt;/enabled&gt;\n            &lt;updatePolicy&gt;never&lt;/updatePolicy&gt;\n            &lt;checksumPolicy&gt;fail&lt;/checksumPolicy&gt;\n            &lt;/snapshots&gt;\n            &lt;id&gt;HDPReleases&lt;/id&gt;\n            &lt;name&gt;HDP Releases&lt;/name&gt;\n            &lt;url&gt;http://repo.hortonworks.com/content/repositories/releases/&lt;/url&gt;\n            &lt;layout&gt;default&lt;/layout&gt;\n        &lt;/repository&gt;\n        &lt;repository&gt;\n            &lt;releases&gt;\n                &lt;enabled&gt;true&lt;/enabled&gt;\n                &lt;updatePolicy&gt;always&lt;/updatePolicy&gt;\n                &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt;\n            &lt;/releases&gt;\n            &lt;snapshots&gt;\n                &lt;enabled&gt;true&lt;/enabled&gt;\n                &lt;updatePolicy&gt;never&lt;/updatePolicy&gt;\n                &lt;checksumPolicy&gt;fail&lt;/checksumPolicy&gt;\n            &lt;/snapshots&gt;\n            &lt;id&gt;HDPPublic&lt;/id&gt;\n            &lt;name&gt;HDP Public&lt;/name&gt;\n            &lt;url&gt;http://repo.hortonworks.com/content/repositories/public/&lt;/url&gt;\n            &lt;layout&gt;default&lt;/layout&gt;\n        &lt;/repository&gt;\n    &lt;/repositories&gt;\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;\n            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;\n            &lt;version&gt;2.7.3.2.5.0.0-1245&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;\n            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;\n            &lt;version&gt;2.7.3.2.5.0.0-1245&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.codehaus.jackson&lt;/groupId&gt;\n            &lt;artifactId&gt;jackson-mapper-asl&lt;/artifactId&gt;\n        &lt;version&gt;1.9.8&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;junit&lt;/groupId&gt;\n            &lt;artifactId&gt;junit&lt;/artifactId&gt;\n            &lt;version&gt;3.8.1&lt;/version&gt;\n            &lt;scope&gt;test&lt;/scope&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n&lt;/project&gt;\n</code></pre>\n<p>Finally it can be compiled and packaged:</p>\n<pre><code class=\"shell\">mvn package\n</code></pre>\n<h2>Add ZipFileInputFormat to Apache Zeppelin</h2>\n<p>Now copy <code>target/com-cotdp-hadoop-1.0-SNAPSHOT.jar</code> to the machine where Zeppeliln Server is installed, e.g. to <code>/tmp/com-cotdp-hadoop-1.0-SNAPSHOT.jar</code></p>\n<p>Open the Interpreter settings under <code>&lt;logged-in-user&gt; - Interpreter</code>, edit <code>/tmp/com-cotdp-hadoop-1.0-SNAPSHOT.jar</code> as an artifact below <code>Dependencies</code></p>\n<p>Restart the Spark Interpreter and <code>ZipFileInputformat</code> can be used</p>\n"},"dateCreated":"2016-11-12T18:50:51+0100","dateStarted":"2016-11-13T15:01:58+0100","dateFinished":"2016-11-13T15:01:58+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7777"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479059936881_-2003525161","id":"20161113-185856_1512573897","dateCreated":"2016-11-13T18:58:56+0100","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6516"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479041920948_-485340020","id":"20161113-135840_1361755711","dateCreated":"2016-11-13T13:58:40+0100","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:5143"}],"name":"spark-unzip","id":"2BXZDDXTX","angularObjects":{"2BZRUZC6W:shared_process":[],"2BZ7ME65H:shared_process":[],"2BZCGXGQJ:shared_process":[],"2BZF9SQ9F:shared_process":[],"2BZGHB8Y9:shared_process":[],"2BYCGD5VM:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}