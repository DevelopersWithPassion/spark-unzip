{"paragraphs":[{"title":"As always, check the Spark version","text":"sc.version","dateUpdated":"2016-11-13T19:30:41+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476888496349_736584997","id":"20161019-164816_1106641995","result":{"code":"SUCCESS","type":"TEXT","msg":"res47: String = 1.6.2\n"},"dateCreated":"2016-10-19T16:48:16+0200","dateStarted":"2016-11-13T19:30:41+0100","dateFinished":"2016-11-13T19:30:42+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:429","focus":true},{"text":"%md\n# 1) ZIP compressed data\n\nZIP compression format is not splittable and there is no default input format defined in Hadoop. To read ZIP files, Hadoop needs to be informed that it this file type is not splittable and needs an appropriate record reader, see [Hadoop: Processing ZIP files in Map/Reduce](http://cutler.io/2012/07/hadoop-processing-zip-files-in-mapreduce/).\n\nIn order to work with ZIP files in Zeppelin, follow the installation instructions in the `Appendix` of this notebook\n\nTest data can be created with `data/create-data.sh`\n","dateUpdated":"2016-11-13T19:50:22+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479041247706_-1949630272","id":"20161113-134727_9559059","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>1) ZIP compressed data</h1>\n<p>ZIP compression format is not splittable and there is no default input format defined in Hadoop. To read ZIP files, Hadoop needs to be informed that it this file type is not splittable and needs an appropriate record reader, see <a href=\"http://cutler.io/2012/07/hadoop-processing-zip-files-in-mapreduce/\">Hadoop: Processing ZIP files in Map/Reduce</a>.</p>\n<p>In order to work with ZIP files in Zeppelin, follow the installation instructions in the <code>Appendix</code> of this notebook</p>\n<p>Test data can be created with <code>data/create-data.sh</code></p>\n"},"dateCreated":"2016-11-13T13:47:27+0100","dateStarted":"2016-11-13T19:50:22+0100","dateFinished":"2016-11-13T19:50:22+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:430","focus":true},{"title":"Six zip files containing XML records are placed below /tmp/zip","text":"%sh\n\necho \"Folder:\"\nhdfs dfs -ls /tmp/zip\necho \" \"\n\necho \"ZIP Files\"\nrm -f /tmp/l.zip\nhdfs dfs -get /tmp/zip/logfiles1.zip /tmp/l.zip\nunzip -l /tmp/l.zip\necho \" \"\n\necho \"XML records:\"\nunzip -p /tmp/l.zip | head -n 3\n","dateUpdated":"2016-11-13T19:45:19+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479044260709_-449418287","id":"20161113-143740_1188005968","result":{"code":"SUCCESS","type":"TEXT","msg":"Folder:\nFound 3 items\n-rw-r--r--   3 bernhard hdfs     712729 2016-11-13 19:43 /tmp/zip/logfiles1.zip\n-rw-r--r--   3 bernhard hdfs     713130 2016-11-13 19:43 /tmp/zip/logfiles2.zip\n-rw-r--r--   3 bernhard hdfs     711966 2016-11-13 19:43 /tmp/zip/logfiles3.zip\n \nZIP Files\nArchive:  /tmp/l.zip\n  Length      Date    Time    Name\n---------  ---------- -----   ----\n  2313784  2016-11-13 19:37   logfile_0\n  2314604  2016-11-13 19:37   logfile_1\n  2313592  2016-11-13 19:37   logfile_2\n---------                     -------\n  6941980                     3 files\n \nXML records:\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><name><first_name>Constance</first_name><last_name>Schaaf</last_name></name><address><city>Neubrandenburg</city><street>Carmine-Löchel-Weg</street><country>Deutschland</country></address></root>\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><name><first_name>Rosalie</first_name><last_name>Lorch</last_name></name><address><city>Rastatt</city><street>Schachtring</street><country>Deutschland</country></address></root>\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><name><first_name>Max</first_name><last_name>Radisch</last_name></name><address><city>Nordhausen</city><street>Ditschlerinweg</street><country>Deutschland</country></address></root>\n"},"dateCreated":"2016-11-13T14:37:40+0100","dateStarted":"2016-11-13T19:45:19+0100","dateFinished":"2016-11-13T19:45:32+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:431","focus":true},{"title":"Empty the results folder","text":"%sh\nhdfs dfs -rm -r /tmp/results\nhdfs dfs -mkdir /tmp/results","dateUpdated":"2016-11-13T19:45:42+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh","tableHide":false,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1478972827127_-2060293394","id":"20161112-184707_1443886968","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-11-12T18:47:07+0100","dateStarted":"2016-11-13T19:45:42+0100","dateFinished":"2016-11-13T19:45:55+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:432","focus":true},{"title":"Import all necessary symbols","text":"import com.cotdp.hadoop.ZipFileInputFormat\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\nimport org.apache.hadoop.io.{BytesWritable, Text}\nimport org.apache.spark.{SparkConf, SparkContext}\nimport scala.xml.XML","dateUpdated":"2016-11-13T19:45:57+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1478536196808_-155352920","id":"20161107-172956_1447712391","result":{"code":"SUCCESS","type":"TEXT","msg":"import com.cotdp.hadoop.ZipFileInputFormat\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{FileStatus, FileSystem, Path}\nimport org.apache.hadoop.io.{BytesWritable, Text}\nimport org.apache.spark.{SparkConf, SparkContext}\nimport scala.xml.XML\n"},"dateCreated":"2016-11-07T17:29:56+0100","dateStarted":"2016-11-13T19:45:57+0100","dateFinished":"2016-11-13T19:45:59+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:433","focus":true},{"title":"Parse an XML record and return it as a scala case class instance","text":"case class Person(first_name:String, last_name:String, street:String, city:String, country:String)\n\ndef parseXML(xmlStr: String) = {\n    val xml = XML.loadString(xmlStr)\n    val first_name = (xml \\ \"name\" \\ \"first_name\").text\n    val last_name = (xml \\ \"name\" \\ \"last_name\").text\n    val street = (xml \\ \"address\" \\ \"street\").text\n    val city = (xml \\ \"address\" \\ \"city\").text\n    val country = (xml \\ \"address\" \\ \"country\").text\n\n    Person(first_name, last_name, street, city, country)\n}","dateUpdated":"2016-11-13T19:46:37+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","tableHide":false,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1478536200215_-796374713","id":"20161107-173000_1460511438","result":{"code":"SUCCESS","type":"TEXT","msg":"defined class Person\nparseXML: (xmlStr: String)Person\n"},"dateCreated":"2016-11-07T17:30:00+0100","dateStarted":"2016-11-13T19:46:37+0100","dateFinished":"2016-11-13T19:46:38+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:434","focus":true},{"title":"Read all zip files into an RDD ... ","text":"val zipFileRDD = sc.newAPIHadoopFile(\"/tmp/zip\", classOf[ZipFileInputFormat],\n                                                 classOf[Text], \n                                                 classOf[BytesWritable], \n                                                 sc.hadoopConfiguration)\n                   .map { case(a,b) => new String( b.getBytes(), \"UTF-8\" ) }\n\n","dateUpdated":"2016-11-13T19:46:41+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","tableHide":false,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1478536549683_129237551","id":"20161107-173549_969433631","result":{"code":"SUCCESS","type":"TEXT","msg":"zipFileRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[62] at map at <console>:53\n"},"dateCreated":"2016-11-07T17:35:49+0100","dateStarted":"2016-11-13T19:46:41+0100","dateFinished":"2016-11-13T19:46:42+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:435","focus":true},{"title":"... and parse them into a DataFrame using the case class","text":"val df = zipFileRDD.flatMap { _.split(\"\\n\") }\n                   .map(parseXML)\n                   .toDF\ndf.count","dateUpdated":"2016-11-13T19:46:53+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479045477918_-445610192","id":"20161113-145757_916046809","result":{"code":"SUCCESS","type":"TEXT","msg":"df: org.apache.spark.sql.DataFrame = [first_name: string, last_name: string, street: string, city: string, country: string]\nres53: Long = 90000\n"},"dateCreated":"2016-11-13T14:57:57+0100","dateStarted":"2016-11-13T19:46:53+0100","dateFinished":"2016-11-13T19:48:28+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:436","focus":true},{"title":"Show the result","text":"df.show(10)\n","dateUpdated":"2016-11-13T19:48:35+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479044524130_-1635631439","id":"20161113-144204_214502717","result":{"code":"SUCCESS","type":"TEXT","msg":"+----------+-----------+--------------------+--------------+-----------+\n|first_name|  last_name|              street|          city|    country|\n+----------+-----------+--------------------+--------------+-----------+\n| Constance|     Schaaf|  Carmine-Löchel-Weg|Neubrandenburg|Deutschland|\n|   Rosalie|      Lorch|         Schachtring|       Rastatt|Deutschland|\n|       Max|    Radisch|      Ditschlerinweg|    Nordhausen|Deutschland|\n|     Ahmed|    Heintze|      Cichoriusgasse|        Amberg|Deutschland|\n|   Antonia|      Gunpf| Kreszentia-Bähr-Weg|     Warendorf|Deutschland|\n|     Janko|  Christoph|Jonas-Röhrdanz-Allee|    Gelnhausen|Deutschland|\n|    Ottmar|     Seidel|Ernestine-Hornich...|     Wunsiedel|Deutschland|\n|   Mattias|Wagenknecht|         Kästergasse|      Arnstadt|Deutschland|\n|    Marika|    Scholtz|       Benthinstraße|       Ilmenau|Deutschland|\n|     Jobst|      Rogge|           Zimmerweg|   Sigmaringen|Deutschland|\n+----------+-----------+--------------------+--------------+-----------+\nonly showing top 10 rows\n\n"},"dateCreated":"2016-11-13T14:42:04+0100","dateStarted":"2016-11-13T19:48:36+0100","dateFinished":"2016-11-13T19:48:36+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:437","focus":true},{"title":"Save the records as ORC and as Parquet files","text":"df.write.format(\"orc\").save(\"/tmp/results/people.orc\")\ndf.write.format(\"parquet\").save(\"/tmp/results/people.parquet\")","dateUpdated":"2016-11-13T19:48:39+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1478536653983_165373300","id":"20161107-173733_135405043","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-11-07T17:37:33+0100","dateStarted":"2016-11-13T19:48:39+0100","dateFinished":"2016-11-13T19:51:50+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:438","focus":true},{"title":"According to the number of partitions in Spark several files will be created by Spark","text":"%sh\nhdfs dfs -ls /tmp/results/people.orc\nhdfs dfs -ls /tmp/results/people.parquet","dateUpdated":"2016-11-13T19:52:02+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1478972702809_448853398","id":"20161112-184502_185340409","result":{"code":"SUCCESS","type":"TEXT","msg":"Found 4 items\n-rw-r--r--   3 zeppelin hdfs          0 2016-11-13 19:50 /tmp/results/people.orc/_SUCCESS\n-rw-r--r--   3 zeppelin hdfs     308634 2016-11-13 19:49 /tmp/results/people.orc/part-r-00000-b1f95464-e242-485c-8e6d-e9a927efbe4a.orc\n-rw-r--r--   3 zeppelin hdfs     308621 2016-11-13 19:49 /tmp/results/people.orc/part-r-00001-b1f95464-e242-485c-8e6d-e9a927efbe4a.orc\n-rw-r--r--   3 zeppelin hdfs     308911 2016-11-13 19:50 /tmp/results/people.orc/part-r-00002-b1f95464-e242-485c-8e6d-e9a927efbe4a.orc\nFound 6 items\n-rw-r--r--   3 zeppelin hdfs          0 2016-11-13 19:51 /tmp/results/people.parquet/_SUCCESS\n-rw-r--r--   3 zeppelin hdfs        548 2016-11-13 19:51 /tmp/results/people.parquet/_common_metadata\n-rw-r--r--   3 zeppelin hdfs       2559 2016-11-13 19:51 /tmp/results/people.parquet/_metadata\n-rw-r--r--   3 zeppelin hdfs     309015 2016-11-13 19:51 /tmp/results/people.parquet/part-r-00000-56a9dae0-b285-40e1-bf2e-2f6beb73b16d.gz.parquet\n-rw-r--r--   3 zeppelin hdfs     309185 2016-11-13 19:51 /tmp/results/people.parquet/part-r-00001-56a9dae0-b285-40e1-bf2e-2f6beb73b16d.gz.parquet\n-rw-r--r--   3 zeppelin hdfs     308872 2016-11-13 19:51 /tmp/results/people.parquet/part-r-00002-56a9dae0-b285-40e1-bf2e-2f6beb73b16d.gz.parquet\n"},"dateCreated":"2016-11-12T18:45:02+0100","dateStarted":"2016-11-13T19:52:03+0100","dateFinished":"2016-11-13T19:52:15+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:439","focus":true},{"title":"Now repartition the DataFrame and store it again as ORC and Parquet","text":" df.repartition(1).write.format(\"orc\").save(\"/tmp/results/people1.orc\")\n df.repartition(1).write.format(\"parquet\").save(\"/tmp/results/people1.parquet\")","dateUpdated":"2016-11-13T19:52:28+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1478972994204_-1040511239","id":"20161112-184954_1381939131","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-11-12T18:49:54+0100","dateStarted":"2016-11-13T19:52:28+0100","dateFinished":"2016-11-13T19:55:39+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:440","focus":true},{"title":"There is one content file per saved ORC or Parquet folder in HDFS now","text":"%sh\n\nhdfs dfs -ls /tmp/results/people1.orc\nhdfs dfs -ls /tmp/results/people1.parquet","dateUpdated":"2016-11-13T19:55:46+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1478973035283_-675507126","id":"20161112-185035_1968360301","result":{"code":"SUCCESS","type":"TEXT","msg":"Found 2 items\n-rw-r--r--   3 zeppelin hdfs          0 2016-11-13 19:54 /tmp/results/people1.orc/_SUCCESS\n-rw-r--r--   3 zeppelin hdfs     858603 2016-11-13 19:54 /tmp/results/people1.orc/part-r-00000-870982f1-bc81-47d3-8868-6e337edf300f.orc\nFound 4 items\n-rw-r--r--   3 zeppelin hdfs          0 2016-11-13 19:55 /tmp/results/people1.parquet/_SUCCESS\n-rw-r--r--   3 zeppelin hdfs        548 2016-11-13 19:55 /tmp/results/people1.parquet/_common_metadata\n-rw-r--r--   3 zeppelin hdfs       1222 2016-11-13 19:55 /tmp/results/people1.parquet/_metadata\n-rw-r--r--   3 zeppelin hdfs     886734 2016-11-13 19:55 /tmp/results/people1.parquet/part-r-00000-6ef20812-6e2f-4356-9837-f15021e2f9a0.gz.parquet\n"},"dateCreated":"2016-11-12T18:50:35+0100","dateStarted":"2016-11-13T19:55:46+0100","dateFinished":"2016-11-13T19:55:58+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:441","focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479064023285_-607730309","id":"20161113-200703_970524120","dateCreated":"2016-11-13T20:07:03+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2249","text":"%md\n\n# 2) GZIP compressed data\n\nFor gzip compressed data a default File input Format exists in Hadoop","dateUpdated":"2016-11-13T20:07:18+0100","dateFinished":"2016-11-13T20:07:15+0100","dateStarted":"2016-11-13T20:07:15+0100","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>2) GZIP compressed data</h1>\n<p>For gzip compressed data a default File input Format exists in Hadoop</p>\n"}},{"text":"%sh\n\necho \"Folder:\"\nhdfs dfs -ls /tmp/gzip\necho \" \"\necho \"XML records:\"\nhdfs dfs -cat /tmp/gzip/logfile_1.gz | gzip -dc | head -n 3","dateUpdated":"2016-11-13T19:57:04+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479045232015_-1150606450","id":"20161113-145352_1878739511","result":{"code":"SUCCESS","type":"TEXT","msg":"Folder:\nFound 9 items\n-rw-r--r--   3 bernhard hdfs     230579 2016-11-13 19:56 /tmp/gzip/logfile_0.gz\n-rw-r--r--   3 bernhard hdfs     231281 2016-11-13 19:56 /tmp/gzip/logfile_1.gz\n-rw-r--r--   3 bernhard hdfs     230567 2016-11-13 19:56 /tmp/gzip/logfile_2.gz\n-rw-r--r--   3 bernhard hdfs     231273 2016-11-13 19:56 /tmp/gzip/logfile_3.gz\n-rw-r--r--   3 bernhard hdfs     230518 2016-11-13 19:56 /tmp/gzip/logfile_4.gz\n-rw-r--r--   3 bernhard hdfs     231085 2016-11-13 19:56 /tmp/gzip/logfile_5.gz\n-rw-r--r--   3 bernhard hdfs     230767 2016-11-13 19:56 /tmp/gzip/logfile_6.gz\n-rw-r--r--   3 bernhard hdfs     230531 2016-11-13 19:56 /tmp/gzip/logfile_7.gz\n-rw-r--r--   3 bernhard hdfs     230467 2016-11-13 19:56 /tmp/gzip/logfile_8.gz\n \nXML records:\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><name><first_name>Abram</first_name><last_name>Loos</last_name></name><address><city>Mellrichstadt</city><street>Mikhail-Scholl-Allee</street><country>Deutschland</country></address></root>\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><name><first_name>Sylvester</first_name><last_name>Hübel</last_name></name><address><city>Eichstätt</city><street>Paulina-Ortmann-Allee</street><country>Deutschland</country></address></root>\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><name><first_name>Sandy</first_name><last_name>Schulz</last_name></name><address><city>Freudenstadt</city><street>Felicia-Hermighausen-Gasse</street><country>Deutschland</country></address></root>\n"},"dateCreated":"2016-11-13T14:53:52+0100","dateStarted":"2016-11-13T19:57:04+0100","dateFinished":"2016-11-13T19:57:17+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:443","focus":true},{"title":"Read all gzip files into an RDD ...","text":"val zipFileRDD2 = sc.textFile(\"/tmp/gzip\")\n","dateUpdated":"2016-11-13T19:57:23+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479045343043_-839793925","id":"20161113-145543_492894261","result":{"code":"SUCCESS","type":"TEXT","msg":"zipFileRDD2: org.apache.spark.rdd.RDD[String] = /tmp/gzip MapPartitionsRDD[84] at textFile at <console>:49\n"},"dateCreated":"2016-11-13T14:55:43+0100","dateStarted":"2016-11-13T19:57:24+0100","dateFinished":"2016-11-13T19:57:24+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:444","focus":true},{"title":"... and (as above for zip files) parse them into a DataFrame using the case class","text":"val df2 = zipFileRDD2.flatMap { _.split(\"\\n\") }\n                     .map(parseXML)\n                     .toDF\ndf.count\n","dateUpdated":"2016-11-13T19:57:26+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479045549298_-1397019246","id":"20161113-145909_661176571","result":{"code":"SUCCESS","type":"TEXT","msg":"df2: org.apache.spark.sql.DataFrame = [first_name: string, last_name: string, street: string, city: string, country: string]\nres64: Long = 90000\n"},"dateCreated":"2016-11-13T14:59:09+0100","dateStarted":"2016-11-13T19:57:26+0100","dateFinished":"2016-11-13T19:58:59+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:445","focus":true},{"text":"df2.show(10)","dateUpdated":"2016-11-13T19:59:14+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479045390697_1188879756","id":"20161113-145630_1813977541","result":{"code":"SUCCESS","type":"TEXT","msg":"+----------+-----------+--------------------+--------------+-----------+\n|first_name|  last_name|              street|          city|    country|\n+----------+-----------+--------------------+--------------+-----------+\n| Constance|     Schaaf|  Carmine-Löchel-Weg|Neubrandenburg|Deutschland|\n|   Rosalie|      Lorch|         Schachtring|       Rastatt|Deutschland|\n|       Max|    Radisch|      Ditschlerinweg|    Nordhausen|Deutschland|\n|     Ahmed|    Heintze|      Cichoriusgasse|        Amberg|Deutschland|\n|   Antonia|      Gunpf| Kreszentia-Bähr-Weg|     Warendorf|Deutschland|\n|     Janko|  Christoph|Jonas-Röhrdanz-Allee|    Gelnhausen|Deutschland|\n|    Ottmar|     Seidel|Ernestine-Hornich...|     Wunsiedel|Deutschland|\n|   Mattias|Wagenknecht|         Kästergasse|      Arnstadt|Deutschland|\n|    Marika|    Scholtz|       Benthinstraße|       Ilmenau|Deutschland|\n|     Jobst|      Rogge|           Zimmerweg|   Sigmaringen|Deutschland|\n+----------+-----------+--------------------+--------------+-----------+\nonly showing top 10 rows\n\n"},"dateCreated":"2016-11-13T14:56:30+0100","dateStarted":"2016-11-13T19:59:14+0100","dateFinished":"2016-11-13T19:59:15+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:446","focus":true},{"text":"%md\n\n# Appendix) Add ZipFileInputFormat to HDP 2.5\n\n## Build ZipFileInputFormat\n\nIn order to get the ZIP Input Format running with HDP 2.5, the repository needs to be downloaded first (`git clone https://github.com/cotdp/com-cotdp-hadoop.git`)\n\nThen `pom.xml` needs to updated to reflect the library versions of HDP 2.5:\n\n```xml\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n    xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n    <groupId>com.cotdp.hadoop</groupId>\n    <artifactId>com-cotdp-hadoop</artifactId>\n    <packaging>jar</packaging>\n    <version>1.0-SNAPSHOT</version>\n    <name>com-cotdp-hadoop</name>\n    <url>http://cotdp.com/</url>\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n    </properties>\n    <repositories>\n        <!-- HDP Repositories -->\n        <repository>\n            <releases>\n                <enabled>true</enabled>\n                <updatePolicy>always</updatePolicy>\n                <checksumPolicy>warn</checksumPolicy>\n            </releases>\n            <snapshots>\n            <enabled>false</enabled>\n            <updatePolicy>never</updatePolicy>\n            <checksumPolicy>fail</checksumPolicy>\n            </snapshots>\n            <id>HDPReleases</id>\n            <name>HDP Releases</name>\n            <url>http://repo.hortonworks.com/content/repositories/releases/</url>\n            <layout>default</layout>\n        </repository>\n        <repository>\n            <releases>\n                <enabled>true</enabled>\n                <updatePolicy>always</updatePolicy>\n                <checksumPolicy>warn</checksumPolicy>\n            </releases>\n            <snapshots>\n                <enabled>true</enabled>\n                <updatePolicy>never</updatePolicy>\n                <checksumPolicy>fail</checksumPolicy>\n            </snapshots>\n            <id>HDPPublic</id>\n            <name>HDP Public</name>\n            <url>http://repo.hortonworks.com/content/repositories/public/</url>\n            <layout>default</layout>\n        </repository>\n    </repositories>\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-common</artifactId>\n            <version>2.7.3.2.5.0.0-1245</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-client</artifactId>\n            <version>2.7.3.2.5.0.0-1245</version>\n        </dependency>\n        <dependency>\n            <groupId>org.codehaus.jackson</groupId>\n            <artifactId>jackson-mapper-asl</artifactId>\n        <version>1.9.8</version>\n        </dependency>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>3.8.1</version>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n</project>\n```\n\nFinally it can be compiled and packaged:\n\n```shell\nmvn package\n```\n\n## Add ZipFileInputFormat to Apache Zeppelin\n\nNow copy `target/com-cotdp-hadoop-1.0-SNAPSHOT.jar` to the machine where Zeppeliln Server is installed, e.g. to `/tmp/com-cotdp-hadoop-1.0-SNAPSHOT.jar`\n\nOpen the Interpreter settings under `<logged-in-user> - Interpreter`, edit `/tmp/com-cotdp-hadoop-1.0-SNAPSHOT.jar` as an artifact below `Dependencies`\n\nRestart the Spark Interpreter and `ZipFileInputformat` can be used\n","dateUpdated":"2016-11-13T15:02:01+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1478973051218_1783453743","id":"20161112-185051_710211259","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Appendix) Add ZipFileInputFormat to HDP 2.5</h1>\n<h2>Build ZipFileInputFormat</h2>\n<p>In order to get the ZIP Input Format running with HDP 2.5, the repository needs to be downloaded first (<code>git clone https://github.com/cotdp/com-cotdp-hadoop.git</code>)</p>\n<p>Then <code>pom.xml</code> needs to updated to reflect the library versions of HDP 2.5:</p>\n<pre><code class=\"xml\">&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n    xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n    &lt;groupId&gt;com.cotdp.hadoop&lt;/groupId&gt;\n    &lt;artifactId&gt;com-cotdp-hadoop&lt;/artifactId&gt;\n    &lt;packaging&gt;jar&lt;/packaging&gt;\n    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;\n    &lt;name&gt;com-cotdp-hadoop&lt;/name&gt;\n    &lt;url&gt;http://cotdp.com/&lt;/url&gt;\n    &lt;properties&gt;\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n    &lt;/properties&gt;\n    &lt;repositories&gt;\n        &lt;!-- HDP Repositories --&gt;\n        &lt;repository&gt;\n            &lt;releases&gt;\n                &lt;enabled&gt;true&lt;/enabled&gt;\n                &lt;updatePolicy&gt;always&lt;/updatePolicy&gt;\n                &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt;\n            &lt;/releases&gt;\n            &lt;snapshots&gt;\n            &lt;enabled&gt;false&lt;/enabled&gt;\n            &lt;updatePolicy&gt;never&lt;/updatePolicy&gt;\n            &lt;checksumPolicy&gt;fail&lt;/checksumPolicy&gt;\n            &lt;/snapshots&gt;\n            &lt;id&gt;HDPReleases&lt;/id&gt;\n            &lt;name&gt;HDP Releases&lt;/name&gt;\n            &lt;url&gt;http://repo.hortonworks.com/content/repositories/releases/&lt;/url&gt;\n            &lt;layout&gt;default&lt;/layout&gt;\n        &lt;/repository&gt;\n        &lt;repository&gt;\n            &lt;releases&gt;\n                &lt;enabled&gt;true&lt;/enabled&gt;\n                &lt;updatePolicy&gt;always&lt;/updatePolicy&gt;\n                &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt;\n            &lt;/releases&gt;\n            &lt;snapshots&gt;\n                &lt;enabled&gt;true&lt;/enabled&gt;\n                &lt;updatePolicy&gt;never&lt;/updatePolicy&gt;\n                &lt;checksumPolicy&gt;fail&lt;/checksumPolicy&gt;\n            &lt;/snapshots&gt;\n            &lt;id&gt;HDPPublic&lt;/id&gt;\n            &lt;name&gt;HDP Public&lt;/name&gt;\n            &lt;url&gt;http://repo.hortonworks.com/content/repositories/public/&lt;/url&gt;\n            &lt;layout&gt;default&lt;/layout&gt;\n        &lt;/repository&gt;\n    &lt;/repositories&gt;\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;\n            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;\n            &lt;version&gt;2.7.3.2.5.0.0-1245&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;\n            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;\n            &lt;version&gt;2.7.3.2.5.0.0-1245&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.codehaus.jackson&lt;/groupId&gt;\n            &lt;artifactId&gt;jackson-mapper-asl&lt;/artifactId&gt;\n        &lt;version&gt;1.9.8&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;junit&lt;/groupId&gt;\n            &lt;artifactId&gt;junit&lt;/artifactId&gt;\n            &lt;version&gt;3.8.1&lt;/version&gt;\n            &lt;scope&gt;test&lt;/scope&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n&lt;/project&gt;\n</code></pre>\n<p>Finally it can be compiled and packaged:</p>\n<pre><code class=\"shell\">mvn package\n</code></pre>\n<h2>Add ZipFileInputFormat to Apache Zeppelin</h2>\n<p>Now copy <code>target/com-cotdp-hadoop-1.0-SNAPSHOT.jar</code> to the machine where Zeppeliln Server is installed, e.g. to <code>/tmp/com-cotdp-hadoop-1.0-SNAPSHOT.jar</code></p>\n<p>Open the Interpreter settings under <code>&lt;logged-in-user&gt; - Interpreter</code>, edit <code>/tmp/com-cotdp-hadoop-1.0-SNAPSHOT.jar</code> as an artifact below <code>Dependencies</code></p>\n<p>Restart the Spark Interpreter and <code>ZipFileInputformat</code> can be used</p>\n"},"dateCreated":"2016-11-12T18:50:51+0100","dateStarted":"2016-11-13T15:01:58+0100","dateFinished":"2016-11-13T15:01:58+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:447"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479059936881_-2003525161","id":"20161113-185856_1512573897","dateCreated":"2016-11-13T18:58:56+0100","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:448"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1479041920948_-485340020","id":"20161113-135840_1361755711","dateCreated":"2016-11-13T13:58:40+0100","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:449"}],"name":"spark-unzip","id":"2BXZDDXTX","angularObjects":{},"config":{"looknfeel":"default"},"info":{}}